# Python analysis template

[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository serves as a personal template for data science projects.

The template comes with the following features/customisations:

- The project is organised as a research compendium (see the [File Structure](#file-structure) section).
- [Visual Studio Code](https://code.visualstudio.com/) is used as an IDE, along with [various extensions](.vscode/extensions.json).
- A [VS Code Dev Container](https://code.visualstudio.com/docs/devcontainers/containers) ([Docker](https://www.docker.com/)) is used as a preferred development environment.
- [pre-commit](https://pre-commit.com/) is used to manage git hooks.
- Python tooling
  - [Black](https://black.readthedocs.io/en/stable/index.html) is used as a formatter (pre-commit and VSC extension).
  - [Ruff](https://docs.astral.sh/ruff/) (pre-commit and VSC extension) and [SonarLint](https://marketplace.visualstudio.com/items?itemName=SonarSource.sonarlint-vscode) (VSC extension) are used as linters.
  - [mypy](https://www.mypy-lang.org/) is used as a type checker (VSC extension).
  - [uv](https://docs.astral.sh/uv/) is used to compile requirements (not as a package manager, yet).
  - [pdoc](https://pdoc.dev/docs/pdoc.html) is used to generate API documentation.
    Python docstrings are written following the [Google docstring format](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) and with the help of the [autoDocstring VSC extension](https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring).
  - Automatic versioning of the local package from git tags using [setuptools_scm](https://setuptools-scm.readthedocs.io/en/stable/), following [semantic versioning](https://semver.org/).
- [SQLFluff](https://sqlfluff.com/) is used as a linter and formatter for SQL files (pre-commit and VSC extension).
- [prettier](https://prettier.io/) (VSC extension) is used as a formatter for YAML, JSON and Markdown files.
- [Taplo](https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml) (VSC extension) as a formatter for TOML files
- [Make](https://www.gnu.org/software/make/) is used as an interface to various utility scripts (see the [Make commands](#make-commands) section).

## File structure

```
.
├── analysis/              # Notebooks and analysis scripts
├── data/                  # Data files (usually git ignored)
├── docs/                  # API documentation (git ignored)
├── results/               # Output files: figures, tables, etc. (git ignored)
├── src/                   # Local Python package
│   ├── __init__.py
│   └── config.py          # Configs, constants, settings
├── tests/                 # Unit tests for src/
│   └── test_*.py
├── .devcontainer/         # VS Code dev container setup
├── .vscode/               # VS Code settings and extensions
├── scripts/               # Utility scripts (e.g. env setup)
├── Makefile               # Utility commands (docs, env, versioning)
├── pyproject.toml         # Package/Project configuration, direct dependencies
├── requirements.txt       # Pinned dependencies (generated)
```

## Development environment

> I can set up the environment differently depending on the project.
> The irrelevant sections can be deleted when using the template.

tldr, the steps to set up the development environment are

1. Set up a virtual environment (venv, conda, Docker, etc.).
2. Install dependencies with `pip install -r requirements.txt` and the local package `src` in editable mode for convenience with `pip install -e .[all]`. A shortcut for this is `make deps`.
3. Install pre-commit hooks with `pre-commit install`.

NB: This steps are automated, for example when using the [`setup_venv.sh`](scripts/setup_venv.sh) script or when using a VS Code Dev Container (see below).

### Requirements

The requirements are specified in the following files:

- [`pyproject.toml`](pyproject.toml) to store the direct dependencies of the `src` package and development dependencies (e.g. for the analysis).
- [`requirements.txt`](requirements.txt) to pin the dependencies (direct and indirect).
  This file is automatically generated with [`uv`](https://docs.astral.sh/uv/) and can then be used to recreate the environment from scratch, e.g. using `pip install -r requirements.txt`.

NB1: the [`requirements.txt`](requirements.txt) file does not include the local package (`src`), hence the two-steps process of installing dependencies.

NB2: When using a conda environment, the dependencies are pinned in an `environment.yml` file instead, see below.

#### Initial setup

1. Start with an empty `requirements.txt`.
2. Install uv with `pip install uv`.
3. Compile requirements with `uv pip compile pyproject.toml -o requirements.txt --all-extras` (or `make reqs`) to generate a `requirements.txt` file.
4. Install requirements with `pip install -r requirements.txt` and then the local package with `pip install -e .[all]`.

#### Update the environment

- To upgrade packages, run `uv pip compile pyproject.toml -o requirements.txt --all-extras --upgrade`.
- To add new packages, add packages in `pyproject.toml` and then compile requirements as above.

### venv setup

Run `scripts/setup_venv.sh` (or `make venv`) to setup a Python virtual environment with [venv](https://docs.python.org/3/library/venv.html), install dependencies in `requirements.txt` and the local package.
By default, the environment is called `.venv` and is created using the default Python interpreter in the current directory.

### Conda setup

To set up the environment with [conda](https://docs.conda.io/projects/conda/en/stable/) (assuming it is already installed), navigate to the repository directory and run `scripts/setup_conda.sh` (specify the Python version and environment name as appropriate with with the `-p` argument):

Then pin the requirements with:

```bash
$ conda env export > environment.yml
```

Finally, the environment can be recreated with:

```bash
$ conda create -n myenv -f environment.yml
```

### VS Code Dev Containers (Docker)

A Docker container can be used as a development environment.
In VS Code, this can be achieved using [Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers), which are configured in the [`.devcontainer`](.devcontainer/) directory.
The environment is automatically built as follows:

1. A Docker image of Python is created with packages installed from `requirements.txt` (except local packages). The Python's version can be edited in the [Dockerfile](Dockerfile).
2. The image is ran in a container and the current directory is mounted.
3. The local packages are installed in the container, along with some VS Code extensions.

To set up the dev container:

1. Install and launch [Docker](https://www.docker.com/).
2. Open the container by using the command palette in VS Code (`Ctrl + Shift + P`) to search for "Dev Containers: Open Folder in Container...".

If needed, the container can be rebuilt by searching for "Dev Containers: Rebuild Container...".

NB: Python packages in `requirements.txt` are installed in the global location of the Docker image.
However, within the container (unless logging in as a root user), packages are installed in the user location and packages in the global location cannot be updated/removed.

#### Private Git packages

If `requirements.txt` contains Python packages in private Git repositories, it is easier to install them in the devcontainer post-creation step since Git credentials used in VSC are shared with the devcontainer (alternatively, credentials have to be made available in the Dockerfile).

One way to achieve this is to exclude git packages from being installed in the Docker image and update the devcontainer post-creation step to install these packages, similarly to how local package are excluded.

For example, in the [Dockerfile](Dockerfile):

```docker
RUN grep -vE '(^-e|@ ?git ?+)' /tmp/pip-tmp/requirements.txt | pip --no-cache-dir install -r /dev/stdin
```

And in [`devcontainer.json`](.devcontainer/devcontainer.json):

```json
"postCreateCommand": "grep -E '(^-e|@ ?git ?+)' requirements.txt | pip install -r /dev/stdin"
```

## Make commands

A Makefile is provided as an interface to various utility scripts:

- `make docs` to generate the package documentation.
- `make venv` to setup a venv environment (see [`scripts/setup_venv.sh`](scripts/setup_venv.sh)).
- `make reqs` to compile requirements.
- `make deps` to install requirements in [`requirements.txt`](requirements.txt) and the local package.
- `make tag` to add and push a new Git tag by incrementing the version.

## Using the template

> This section can be deleted when using the template.

### Getting started

1. Initialise your GitHub repository with this template. Alternatively, fork (or copy the content of) this repository.
2. Update
   - [ ] project information in [`pyproject.toml`](pyproject.toml), such as the description and the authors.
   - [ ] the repository name (if the template was forked).
   - [ ] the README (title, badges, sections).
   - [ ] the license.
3. Set up your preferred development environment (see the [Development Environment section](#development-environment)).
4. Add a git tag for the inital version with `git tag -a v0.1.0 -m "Initial setup"`, and push it with `git push origin --tags`. Alternatively, use `make tag`.

### Possible extensions

The `src/` package could contain the following modules or sub-packages depending on the project:

- `utils` for utility functions.
- `data_processing` or `data` for data processing functions.
- `features`: for extracting features.
- `models`: for defining models.
- `evaluation`: for evaluating performance.
- `plots`: for plotting functions.

The repository structure could be extended with:

- subfolders in `data/` such as `data/raw/` for storing raw data.
- `models/` to store model files.

Finally, a full project documentation (beyond the API) could be generated using [mkdocs](https://www.mkdocs.org/) or [quartodoc](https://machow.github.io/quartodoc/get-started/overview.html).

### Related

This template is inspired by the concept of a [research compendium](https://doi.org/10.1080/00031305.2017.1375986) and similar projects I created for R projects (e.g. [reproducible-workflow](https://github.com/ghurault/reproducible-workflow)).

This template is relatively simple and tailored to my needs.
More sophisticated templates are available elsewhere, such as:

- [Cookiecutter Data Science](https://github.com/drivendataorg/cookiecutter-data-science/).
- [https://joserzapata.github.io/data-science-project-template/](https://joserzapata.github.io/data-science-project-template/)
- [Data Science for Social Good's hitchhikers guide template](https://github.com/dssg/hitchhikers-guide/tree/master/sources/curriculum/0_before_you_start/pipelines-and-project-workflow)
- [https://github.com/khuyentran1401/data-science-template](https://github.com/khuyentran1401/data-science-template)

As opposed to other templates, this template is more focused on experimentation rather than sharing a single final product.
